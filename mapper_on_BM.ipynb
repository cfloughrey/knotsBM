{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.sparse import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyBallMapper_Bokeh import graph_GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to deal with large csv\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10\n",
    "    # as long as the OverflowError occurs.\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Read the graphs\n",
    "\n",
    "# Each graph must be rapresented by an adjecency list (space separated)\n",
    "# We assume nodes are numbered from 1 to N\n",
    "#\n",
    "# The list of points covereb by each node is a file with N lines, each line contains the points id (space separated)\n",
    "\n",
    "def read_graph_from_list(GRAPH_ADJ_PATH, GRAPH_POINTS_PATH):\n",
    "    # read graph adjecency list\n",
    "    # G_dummy is needed because I want the nodes to be ordered\n",
    "    # ASSUME NODES ARE NUMBERED FROM 1 TO N\n",
    "    G_dummy = nx.read_adjlist(GRAPH_ADJ_PATH, nodetype = int,\n",
    "                              delimiter=' ')\n",
    "\n",
    "    # read list of points covered by each node\n",
    "    # ASSUME NODES ARE NUMBERED FROM 1 TO N\n",
    "    csv_file = open(GRAPH_POINTS_PATH)\n",
    "    reader = csv.reader(csv_file)\n",
    "\n",
    "    points_covered = {}\n",
    "    MAX_NODE_SIZE = 0\n",
    "    for i, line_list in enumerate(reader):\n",
    "        points_covered[i+1] = [int(node) for node in line_list[0].split(' ')]\n",
    "        if len(points_covered[i+1]) > MAX_NODE_SIZE:\n",
    "            MAX_NODE_SIZE = len(points_covered[i+1])\n",
    "\n",
    "    # add the nodes that are not in the edgelist\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from( range(1, len(points_covered) + 1) )\n",
    "    G.add_edges_from(G_dummy.edges)\n",
    "\n",
    "    MIN_SCALE = 7\n",
    "    MAX_SCALE = 20\n",
    "\n",
    "    for node in G.nodes:\n",
    "        G.nodes[node]['points covered R'] = points_covered[node]\n",
    "        G.nodes[node]['points covered'] = (np.array(points_covered[node])-1).tolist()\n",
    "        G.nodes[node]['size'] = len(G.nodes[node]['points covered'])\n",
    "        # rescale the size for display\n",
    "        G.nodes[node]['size rescaled'] = MAX_SCALE*G.nodes[node]['size']/MAX_NODE_SIZE + MIN_SCALE\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper on BM using DBscan as clustering algo\n",
    "# it uses scipy csr sparse matrix to speed up computations\n",
    "# inputs:\n",
    "#     BM_J        ball mapper graph\n",
    "#     dense_K_df  pandas dataframe where to pull back elements in the BM\n",
    "#     EPS         radius for the DBscan algo\n",
    "#     MIN_SAMPLES min number of elements in a cluster that make it a cluster and not noise\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "def mapper_on_BM(BM_J, dense_K_df, EPS, MIN_SAMPLES=1):\n",
    "    new_graph = nx.Graph()\n",
    "\n",
    "    # creates a sparse CSR matrix\n",
    "    sparse_K = csr_matrix(dense_K_df.values)\n",
    "\n",
    "    for node in tqdm(BM_J.nodes):\n",
    "        X = sparse_K[BM_J.nodes[node]['points covered'], :]\n",
    "\n",
    "        db = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES).fit(X)\n",
    "        # create a set of unique labels\n",
    "        labels = set(db.labels_) - {-1} # outliers are not clusters\n",
    "\n",
    "        #print('\\n **********')\n",
    "        #print(node, X.shape[0], labels)\n",
    "\n",
    "        # for each cluster\n",
    "        # add a new vertex to the new graph\n",
    "        for cluster in labels:\n",
    "            # print the number of points in the cluster\n",
    "            #print('\\t', cluster, (db.labels_ == cluster).sum())\n",
    "            # retrives the indeces of the points covered by the cluster\n",
    "            points_covered_by_cluster = np.array(BM_J.nodes[node]['points covered'])[np.where(db.labels_\n",
    "                                                                                     == cluster)].tolist()\n",
    "            # creates a node\n",
    "            new_graph.add_node(str(node)+'_'+str(cluster),\n",
    "                               points_covered=points_covered_by_cluster)\n",
    "\n",
    "        for neigh in [v for v in nx.neighbors(BM_J, node) if v > node]:\n",
    "            neigh_X = sparse_K[BM_J.nodes[neigh]['points covered'], :]\n",
    "\n",
    "            neigh_db = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES).fit(neigh_X)\n",
    "            neigh_labels = set(neigh_db.labels_) - {-1} # outliers are not clusters\n",
    "\n",
    "            # add edges between clusters that belongs to neigh in the original graph\n",
    "            # if they share at least one element\n",
    "            for cluster in labels:\n",
    "                for neigh_cluster in neigh_labels:\n",
    "                    points_covered_by_cluster = np.array(BM_J.nodes[node]['points covered'])[np.where(db.labels_\n",
    "                                                                                             == cluster)].tolist()\n",
    "                    points_covered_by_neigh=np.array(BM_J.nodes[neigh]['points covered'])[np.where(neigh_db.labels_\n",
    "                                                                                          == neigh_cluster)].tolist()\n",
    "                    if len( set(points_covered_by_cluster)&set(points_covered_by_neigh) ) != 0:\n",
    "                        new_graph.add_edge(str(node)+'_'+str(cluster), str(neigh)+'_'+str(neigh_cluster) )\n",
    "\n",
    "\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will save the mapper_on_BM to disk as pickle files \n",
    "# this way we can plot them in a second moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the mapper_on_BM graph from pickle\n",
    "\n",
    "def read_graph_from_pickle(GRAPH_PATH,\n",
    "                           values_df,\n",
    "                           my_palette):\n",
    "    # read graph \n",
    "    G = nx.read_gpickle(GRAPH_PATH)\n",
    "    \n",
    "    MIN_SCALE = 7\n",
    "    MAX_SCALE = 20\n",
    "\n",
    "    MAX_NODE_SIZE = 0\n",
    "    for node in G.nodes:\n",
    "        if len(G.nodes[node]['points_covered']) > MAX_NODE_SIZE:\n",
    "            MAX_NODE_SIZE = len(G.nodes[node]['points_covered'])\n",
    "\n",
    "    for node in G.nodes:\n",
    "        G.nodes[node]['size'] = len(G.nodes[node]['points_covered'])\n",
    "        # rescale the size for display\n",
    "        G.nodes[node]['size rescaled'] = MAX_SCALE*G.nodes[node]['size']/MAX_NODE_SIZE + MIN_SCALE\n",
    "\n",
    "        G.nodes[node]['color'] = my_palette(0)\n",
    "\n",
    "        for name, avg in values_df.loc[G.nodes[node]['points_covered']].mean().iteritems():\n",
    "            G.nodes[node][name] = avg\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE\n",
    "## Jones upto13n TO Khovanov upto13n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('up to 13n SYMM 20 JONES')\n",
    "\n",
    "BM_J_13n = read_graph_from_list('output/jones_fromK_upto_13n_MIRRORS/SYM_20_edges',\n",
    "                               'output/jones_fromK_upto_13n_MIRRORS/SYM_20_points_covered_by_landmarks')\n",
    "\n",
    "K_13n_df = pd.read_csv('output/jones_fromK_upto_13n_MIRRORS/Khovanov_upto_13n_MIRRORS_coeff.csv', \n",
    "                       sep=' ')\n",
    "\n",
    "print('data loaded')\n",
    "print('computing mapper on BM')\n",
    "pullback_13n_15 = mapper_on_BM(BM_J_13n, K_13n_df,\n",
    "                               EPS=15, MIN_SAMPLES=1)\n",
    "\n",
    "nx.write_gpickle(pullback_13n_15, 'pullback_13n_15_no_thrs_SYMM.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import FixedTicker, LinearColorMapper, LogColorMapper, ColorBar, BasicTicker, LogTicker\n",
    "from matplotlib.colors import to_hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH1_PATH = 'pullback_13n_15_no_thrs_SYMM.gpickle'\n",
    "\n",
    "# table with the coloring functions\n",
    "coloring_df = pd.read_csv('output/jones_fromK_upto_13n_MIRRORS/Khovanov_upto_13n_MIRRORS_colors.csv',\n",
    "                          sep=' ')\n",
    "coloring_df.index = range(len(coloring_df))\n",
    "\n",
    "coloring_df['signature mod4'] = coloring_df.signature % 4\n",
    "\n",
    "###########\n",
    "# GRAPH 1 #\n",
    "###########\n",
    "\n",
    "#Here we adopt standard colour palette\n",
    "my_red_palette = cm.get_cmap(name='jet')\n",
    "\n",
    "# read graph\n",
    "# ASSUME NODES ARE NUMBERED FROM 1 TO N\n",
    "G1 = read_graph_from_pickle(GRAPH1_PATH, coloring_df, my_red_palette)\n",
    "\n",
    "for node in G1.nodes:\n",
    "    G1.nodes[node]['points covered'] = G1.nodes[node]['points_covered']\n",
    "print('loaded graph with {} nodes and {} edges'.format(len(G1.nodes), len(G1.edges)))\n",
    "\n",
    "# create a GUI with input our BM graph, \n",
    "# a dataframe with coloring functions (one value per point in the pointcloud)\n",
    "# and a color palette\n",
    "# in this case we use the pointcloud as coloring function\n",
    "my_fancy_gui = graph_GUI(G1, my_red_palette, coloring_df[['signature']])\n",
    "my_fancy_gui.color_by_variable('signature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete colorbar\n",
    "num_ticks = 9\n",
    "low = -9\n",
    "high = 9\n",
    "color_mapper = LinearColorMapper(palette=[to_hex(my_red_palette(color_id)) \n",
    "                                          for color_id in np.linspace(0, 1, num_ticks)], \n",
    "                                 low=low, high=high)\n",
    "\n",
    "ticks = [i for i in range(-12, 13, 2)]\n",
    "color_ticks = FixedTicker(ticks=ticks)\n",
    "\n",
    "color_bar = ColorBar(color_mapper=color_mapper, \n",
    "                     major_label_text_font_size='14pt',\n",
    "                     label_standoff=12,\n",
    "                     ticker=color_ticks,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fancy_gui.plot.add_layout(color_bar, 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates an html file with the graph \n",
    "# and opens it in another tab\n",
    "show(my_fancy_gui.plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
